"""å¼‚æ­¥APIå®¢æˆ·ç«¯æ¨¡å— - é«˜æ€§èƒ½ç½‘ç»œè¯·æ±‚å¤„ç†"""

import asyncio
import logging
from typing import Any, Dict, Optional

import aiohttp
from cachetools import TTLCache
from tenacity import (
    AsyncRetrying,
    RetryError,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from constant import (
    ASYNC_TIMEOUT,
    CACHE_MAX_SIZE,
    CACHE_TTL,
    CONNECTOR_LIMIT,
    CONNECTOR_LIMIT_PER_HOST,
    HTTP_HEADERS,
    RETRY_TIMES,
    RETRY_INTERVAL,
)


class APIClient:
    """å¼‚æ­¥APIå®¢æˆ·ç«¯ï¼Œæ”¯æŒè¿æ¥æ± ã€ç¼“å­˜å’Œæ™ºèƒ½é‡è¯•"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.session: Optional[aiohttp.ClientSession] = None
        self.cache: TTLCache = TTLCache(maxsize=CACHE_MAX_SIZE, ttl=CACHE_TTL)
        self.request_count = 0
        self.cache_hits = 0

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()

    async def initialize(self):
        """åˆå§‹åŒ–ä¼šè¯å’Œè¿æ¥æ± """
        connector = aiohttp.TCPConnector(
            limit=CONNECTOR_LIMIT,
            limit_per_host=CONNECTOR_LIMIT_PER_HOST,
            ttl_dns_cache=300,
        )
        timeout = aiohttp.ClientTimeout(total=ASYNC_TIMEOUT)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=HTTP_HEADERS,
        )
        self.logger.debug("âœ¨ å¼‚æ­¥APIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")

    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()
            self.logger.debug("âœ¨ å¼‚æ­¥APIå®¢æˆ·ç«¯å·²å…³é—­")
        self.logger.info(
            f"ğŸ“Š APIç»Ÿè®¡ - è¯·æ±‚æ€»æ•°: {self.request_count}, "
            f"ğŸ“Š ç¼“å­˜å‘½ä¸­: {self.cache_hits}, "
            f"ğŸ“Š å‘½ä¸­ç‡: {self.cache_hits / max(self.request_count, 1) * 100:.1f}%"
        )

    async def request(
            self,
            method: str,
            url: str,
            **kwargs,
    ) -> Optional[Dict[str, Any]]:
        """å‘é€HTTPè¯·æ±‚ï¼Œæ”¯æŒç¼“å­˜å’Œé‡è¯•

        Args:
            method: HTTPæ–¹æ³• (GET, POSTç­‰)
            url: è¯·æ±‚URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°

        Returns:
            å“åº”JSONæˆ–None
        """
        self.request_count += 1

        # æ£€æŸ¥ç¼“å­˜ï¼ˆä»…GETè¯·æ±‚ï¼‰
        if method.upper() == "GET" and url in self.cache:
            self.cache_hits += 1
            self.logger.debug(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {url}")
            return self.cache[url]

        if not self.session:
            await self.initialize()

        try:
            async for attempt in AsyncRetrying(
                    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
                    stop=stop_after_attempt(RETRY_TIMES),
                    wait=wait_exponential(multiplier=RETRY_INTERVAL, min=1, max=10),
                    reraise=True,
            ):
                with attempt:
                    async with self.session.request(method, url, **kwargs) as response:
                        if response.status == 200:
                            data = await response.json()

                            # ç¼“å­˜æˆåŠŸå“åº”
                            if method.upper() == "GET":
                                self.cache[url] = data

                            return data
                        elif response.status == 429:  # Rate limit
                            reset_time = response.headers.get("X-RateLimit-Reset")
                            self.logger.warning(f"â±ï¸ APIé€Ÿç‡é™åˆ¶ï¼Œé‡ç½®æ—¶é—´: {reset_time}")
                            raise aiohttp.ClientError("Rate limited")
                        else:
                            self.logger.warning(
                                f"âš ï¸ è¯·æ±‚å¤±è´¥ [{response.status}]: {url}"
                            )
                            raise aiohttp.ClientError(f"HTTP {response.status}")

        except RetryError as e:
            self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥ï¼ˆå·²é‡è¯•{RETRY_TIMES}æ¬¡ï¼‰: {url}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None

    async def get(self, url: str, **kwargs) -> Optional[Dict[str, Any]]:
        """å‘é€GETè¯·æ±‚"""
        return await self.request("GET", url, **kwargs)

    async def raw_get(self, url: str) -> Optional[bytes]:
        """è·å–åŸå§‹äºŒè¿›åˆ¶å†…å®¹ï¼ˆç”¨äºä¸‹è½½æ–‡ä»¶ï¼‰"""
        self.request_count += 1

        if not self.session:
            await self.initialize()

        try:
            async for attempt in AsyncRetrying(
                    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
                    stop=stop_after_attempt(RETRY_TIMES),
                    wait=wait_exponential(multiplier=RETRY_INTERVAL, min=1, max=10),
                    reraise=True,
            ):
                with attempt:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            return await response.read()
                        else:
                            raise aiohttp.ClientError(f"HTTP {response.status}")

        except RetryError:
            self.logger.error(f"âŒ ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•{RETRY_TIMES}æ¬¡ï¼‰: {url}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {str(e)}")
            return None

    async def batch_get(self, urls: list[str], semaphore: Optional[asyncio.Semaphore] = None) -> Dict[
        str, Optional[Dict]]:
        """æ‰¹é‡GETè¯·æ±‚ï¼Œæ”¯æŒå¹¶å‘æ§åˆ¶

        Args:
            urls: URLåˆ—è¡¨
            semaphore: ä¿¡å·é‡ï¼Œç”¨äºé™åˆ¶å¹¶å‘æ•°

        Returns:
            {url: response} å­—å…¸
        """
        if semaphore is None:
            from constant import MAX_WORKERS
            semaphore = asyncio.Semaphore(MAX_WORKERS)

        async def fetch_with_semaphore(url):
            async with semaphore:
                return url, await self.get(url)

        tasks = [fetch_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)

        return {url: response for url, response in results}

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.logger.debug("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")
